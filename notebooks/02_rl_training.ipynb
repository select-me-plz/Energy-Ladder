{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac6b87d",
   "metadata": {},
   "source": [
    "## Factory Energy Environment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22582a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactoryEnergyEnv(gym.Env):\n",
    "    \"\"\"Custom Gym environment for Factory I/O energy optimization.\"\"\"\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, machine_config, max_steps=3600):\n",
    "        super(FactoryEnergyEnv, self).__init__()\n",
    "        \n",
    "        self.machine_config = machine_config\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Define action and observation spaces\n",
    "        self.action_space = self.define_hybrid_action_space()\n",
    "        self.observation_space = self.define_observation_space()\n",
    "        \n",
    "        # Initialize state\n",
    "        self.state = None\n",
    "        self.episode_reward = 0\n",
    "        self.episode_energy = 0\n",
    "    \n",
    "    def define_hybrid_action_space(self):\n",
    "        \"\"\"Define action space: discrete choices for machine sequencing.\"\"\"\n",
    "        # Actions: which machines to prioritize (0-7 represents different combinations)\n",
    "        return spaces.Discrete(8)\n",
    "    \n",
    "    def define_observation_space(self):\n",
    "        \"\"\"Define observation space: factory state variables.\"\"\"\n",
    "        # Observations: 10 dimensions\n",
    "        # [conveyor_power, pusher_power, robot_power, total_power, \n",
    "        #  conveyor_load, pusher_load, robot_load, timestamp, energy_consumed, reward]\n",
    "        return spaces.Box(low=0, high=100, shape=(10,), dtype=np.float32)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_energy = 0\n",
    "        \n",
    "        # Initialize state with random values\n",
    "        self.state = np.array([\n",
    "            np.random.uniform(5, 15),     # conveyor_power\n",
    "            np.random.uniform(2, 8),      # pusher_power\n",
    "            np.random.uniform(5, 12),     # robot_power\n",
    "            0,                             # total_power (will be calculated)\n",
    "            np.random.uniform(0.3, 0.9),  # conveyor_load\n",
    "            np.random.uniform(0.2, 0.8),  # pusher_load\n",
    "            np.random.uniform(0.3, 0.9),  # robot_load\n",
    "            0,                             # timestamp\n",
    "            0,                             # energy_consumed\n",
    "            0                              # last_reward\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        self.state[3] = self.state[0] + self.state[1] + self.state[2]  # total_power\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one step in the environment.\"\"\"\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Modify loads based on action\n",
    "        load_adjustment = (action - 3.5) * 0.05  # Actions 0-7 map to -0.175 to +0.175\n",
    "        \n",
    "        self.state[4] = np.clip(self.state[4] + load_adjustment, 0.1, 1.0)  # conveyor_load\n",
    "        self.state[5] = np.clip(self.state[5] - load_adjustment * 0.5, 0.1, 1.0)  # pusher_load\n",
    "        self.state[6] = np.clip(self.state[6] - load_adjustment * 0.7, 0.1, 1.0)  # robot_load\n",
    "        \n",
    "        # Calculate power based on loads\n",
    "        self.state[0] = 15 * self.state[4]  # conveyor_power\n",
    "        self.state[1] = 8 * self.state[5]   # pusher_power\n",
    "        self.state[2] = 12 * self.state[6]  # robot_power\n",
    "        self.state[3] = self.state[0] + self.state[1] + self.state[2]  # total_power\n",
    "        \n",
    "        # Add noise\n",
    "        self.state += np.random.normal(0, 0.5, size=self.state.shape)\n",
    "        self.state = np.clip(self.state, 0, 100).astype(np.float32)\n",
    "        \n",
    "        # Update timing\n",
    "        self.state[7] = self.current_step  # timestamp\n",
    "        self.state[8] += self.state[3] * 0.1 / 3600  # energy_consumed (cumulative, in kWh)\n",
    "        \n",
    "        # Calculate reward (lower energy is better)\n",
    "        energy_reward = -self.state[3] / 40  # Normalize to [-1, 0]\n",
    "        load_penalty = -0.1 * abs(sum(self.state[4:7]) - 1.5)  # Encourage balanced loads\n",
    "        reward = energy_reward + load_penalty\n",
    "        \n",
    "        self.state[9] = reward\n",
    "        self.episode_reward += reward\n",
    "        self.episode_energy += self.state[3] * 0.1 / 3600\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.current_step >= self.max_steps\n",
    "        \n",
    "        info = {\n",
    "            'total_power': float(self.state[3]),\n",
    "            'energy_consumed': float(self.state[8]),\n",
    "            'episode_reward': float(self.episode_reward),\n",
    "            'conveyor_load': float(self.state[4]),\n",
    "            'pusher_load': float(self.state[5]),\n",
    "            'robot_load': float(self.state[6])\n",
    "        }\n",
    "        \n",
    "        return self.state, reward, done, info\n",
    "    \n",
    "    def calculate_power_consumption(self, actions):\n",
    "        \"\"\"Calculate total power consumption based on actions.\"\"\"\n",
    "        return float(self.state[3])\n",
    "    \n",
    "    def calculate_reward(self, total_power, actions):\n",
    "        \"\"\"Calculate reward signal.\"\"\"\n",
    "        return -total_power / 40\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment (optional).\"\"\"\n",
    "        print(f\"Step {self.current_step}: Power={self.state[3]:.2f}kW, Energy={self.state[8]:.4f}kWh, Reward={self.state[9]:.4f}\")\n",
    "\n",
    "print(\"FactoryEnergyEnv class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c2422",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e79a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factory specifications\n",
    "factory_specs = {\n",
    "    'conveyors': {'count': 14, 'max_power': 28},\n",
    "    'pushers': {'count': 4, 'max_power': 12},\n",
    "    'pick_place': {'count': 3, 'max_power': 12}\n",
    "}\n",
    "\n",
    "# Create environment\n",
    "env = FactoryEnergyEnv(factory_specs, max_steps=3600)\n",
    "\n",
    "# Reset and verify\n",
    "initial_state = env.reset()\n",
    "print(f\"Initial state shape: {initial_state.shape}\")\n",
    "print(f\"Initial state: {initial_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5177c448",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorized environment for stable-baselines3\n",
    "vec_env = DummyVecEnv([lambda: FactoryEnergyEnv(factory_specs, max_steps=3600)])\n",
    "\n",
    "# Define checkpoint callback\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10000,\n",
    "    save_path='./checkpoints/',\n",
    "    name_prefix='rl_model'\n",
    ")\n",
    "\n",
    "# Create PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    vec_env,\n",
    "    verbose=1,\n",
    "    learning_rate=3e-4,\n",
    "    batch_size=64,\n",
    "    n_steps=2048,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95\n",
    ")\n",
    "\n",
    "print(\"PPO model created. Ready for training.\")\n",
    "print(f\"Policy network: {model.policy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3fa62",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e70282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 100,000 timesteps\n",
    "total_timesteps = 100000\n",
    "print(f\"Starting training for {total_timesteps} timesteps...\")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=total_timesteps,\n",
    "    callback=checkpoint_callback,\n",
    "    log_interval=100\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d4e33",
   "metadata": {},
   "source": [
    "## Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = os.path.join('..', 'models')\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(models_dir, 'trained_rl_model')\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e046c1",
   "metadata": {},
   "source": [
    "## Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec254dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "test_env = FactoryEnergyEnv(factory_specs, max_steps=100)\n",
    "obs = test_env.reset()\n",
    "\n",
    "total_energy = 0\n",
    "episode_rewards = []\n",
    "\n",
    "for step in range(100):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = test_env.step(action)\n",
    "    total_energy += info['energy_consumed']\n",
    "    episode_rewards.append(reward)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(f\"Test Results:\")\n",
    "print(f\"  Steps completed: {step + 1}\")\n",
    "print(f\"  Total energy consumed: {total_energy:.4f} kWh\")\n",
    "print(f\"  Average reward per step: {np.mean(episode_rewards):.4f}\")\n",
    "print(f\"  Min power (kW): {min([test_env.state[3]]):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
